name: Scrape Tennis Blog HTML

on:
  workflow_dispatch:
  schedule:
    - cron: "17 3 * * *" # daily at 03:17 UTC

permissions:
  contents: write

concurrency:
  group: scrape-tennis-blog
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install Playwright
        run: |
          npm init -y
          npm i playwright@1
          npx playwright install --with-deps chromium

      - name: Write crawler script
        run: |
          mkdir -p scripts
          cat > scripts/scrape_blog.mjs << 'JS'
          import fs from "node:fs/promises";
          import path from "node:path";
          import { URL } from "node:url";
          import { chromium } from "playwright";

          const START_URL = process.env.START_URL || "https://blog.theloveoftennis.co.uk/";
          const start = new URL(START_URL);
          const BASE_ORIGIN = start.origin;
          const HOST = start.host;
          const OUT_DIR = path.join("snapshots", HOST);

          const ASSET_EXT = [
            ".pdf",".jpg",".jpeg",".png",".gif",".webp",".svg",
            ".mp3",".mp4",".zip",".css",".js",".woff",".woff2",".ttf",".eot",".ico"
          ];

          const seen = new Set();
          const queue = [start.href];

          const sleep = ms => new Promise(r => setTimeout(r, ms));

          function sameOrigin(u) {
            try { return new URL(u, BASE_ORIGIN).origin === BASE_ORIGIN; }
            catch { return false; }
          }

          function stripHash(u) {
            const url = new URL(u, BASE_ORIGIN);
            url.hash = "";
            return url.href;
          }

          function looksLikeAsset(u) {
            const p = new URL(u, BASE_ORIGIN).pathname.toLowerCase();
            return ASSET_EXT.some(ext => p.endsWith(ext));
          }

          // ---- NEW: skip rules ----
          function shouldSkip(u) {
            const path = new URL(u, BASE_ORIGIN).pathname;
            return path.startsWith("/podcast") || path.startsWith("/events");
          }
          // --------------------------

          function urlToFilePath(u) {
            const { pathname, search } = new URL(u);
            let p = pathname;
            if (p === "" || p === "/") p = "/index.html";
            else if (path.extname(p) === "") p = path.posix.join(p, "index.html");

            let rel = p.startsWith("/") ? p.slice(1) : p;
            if (search) {
              const safe = search.replace(/[^\w]+/g, "_").replace(/^_+|_+$/g, "");
              rel = rel.replace(/\.html$/, `_${safe}.html`);
            }
            return path.join(OUT_DIR, rel);
          }

          async function ensureDirFor(filePath) {
            await fs.mkdir(path.dirname(filePath), { recursive: true });
          }

          async function saveFile(filePath, contents) {
            await ensureDirFor(filePath);
            await fs.writeFile(filePath, contents, "utf8");
          }

          async function crawl() {
            const browser = await chromium.launch({ headless: true });
            const context = await browser.newContext({
              userAgent: "GitHubActionsPlaywrightArchiver/1.0"
            });
            const page = await context.newPage();

            const discovered = new Set();

            while (queue.length) {
              const current = stripHash(queue.shift());
              if (seen.has(current)) continue;
              if (!sameOrigin(current) || looksLikeAsset(current) || shouldSkip(current)) continue;
              seen.add(current);

              try {
                await page.goto(current, { waitUntil: "networkidle", timeout: 60000 });

                await page.waitForFunction(() => {
                  const b = document.body;
                  if (!b) return false;
                  const text = b.innerText || "";
                  return text.trim().length > 40 || b.querySelector("*") !== null;
                }, { timeout: 20000 });

                await sleep(500);

                await page.evaluate(async () => {
                  const scroll = () => new Promise(r => {
                    let y = 0;
                    const i = setInterval(() => {
                      const max = document.documentElement.scrollHeight || document.body.scrollHeight || 0;
                      window.scrollTo(0, y += 800);
                      if (y >= max) { clearInterval(i); r(); }
                    }, 50);
                  });
                  await scroll();
                });
                await sleep(300);

                const html = await page.content();
                const filePath = urlToFilePath(current);
                await saveFile(filePath, html);

                const hrefs = await page.$$eval("a[href]", as => as.map(a => a.getAttribute("href")).filter(Boolean));
                for (const href of hrefs) {
                  try {
                    const abs = stripHash(new URL(href, current).href);
                    if (sameOrigin(abs) && !looksLikeAsset(abs) && !seen.has(abs) && !shouldSkip(abs)) {
                      discovered.add(abs);
                    }
                  } catch {}
                }

                const newlyFound = Array.from(discovered).sort();
                discovered.clear();
                queue.push(...newlyFound);
              } catch (err) {
                console.error("Failed:", current, err.message);
              }

              await sleep(200);
            }

            await browser.close();

            const manifestPath = path.join(OUT_DIR, "manifest.txt");
            await ensureDirFor(manifestPath);
            await saveFile(manifestPath, Array.from(seen).sort().join("\n"));
            console.log(`Captured ${seen.size} pages into ${OUT_DIR}`);
          }

          crawl().catch(e => {
            console.error(e);
            process.exit(1);
          });
          JS

      - name: Run crawler
        env:
          START_URL: https://blog.theloveoftennis.co.uk/
        run: node scripts/scrape_blog.mjs

      - name: Commit updates
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore: update rendered blog HTML"
          file_pattern: snapshots/**
